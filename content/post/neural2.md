---
title: "ニューラルネットワークと深層学習のお勉強(2)"
date: 2018-12-30T00:32:14+09:00
draft: false
tags: ["Neural Network","機械学習","Python3"]
categories: ["機械学習","Python3"]
---

## 逆伝播
コスト関数の勾配を高速に計算するアルゴリズム

逆伝播の本質はコスト関数$C$のネットワークの重み$w$もしくはバイアス$b$に関する偏微分$\frac{\partial C}{\partial w}$もしくは$\frac{\partial C}{\partial b}$。
重みとバイアスを変化させた時のコスト関数の変化の度合いがわかる。
逆伝播は単なる高速な学習アルゴリズムではなく、逆伝播を見ることで重みやバイアスを変化させた時のニューラルネットワーク全体の挙動の変化に関して深い洞察が得られる。
わざわざ時間をかけて理解する価値はそこにある。
逆伝播について深く理解しなくともブラックボックスとして扱うことはできる。

## 表記方法

$ w ^ l\_{jk}$で$(l-1)$番目の層の$k$番目のニューロンから$l$番目の層の$j$番目のニューロンへの接続に対する重みを表している。
$j$と$k$の順番に注意。

$ b ^ l _ j $で$l$番目の層の$j$番目のニューロンのバイアスを表す。
$ a ^ l _ j $で$l$番目の層の$j$番目のニューロンの活性を表す。

各層$l$に対し重み行列$w ^ l$。$l$番目の層のニューロンを終点とする接続の重み。
各層$l$に対しバイアスベクトル$b ^ l$。
各層$l$に対し活性ベクトル$a ^ l$。

これらを用いてニューロンの活性化は次式のようにコンパクトに表せる

$$
a ^ l = σ \( w ^ l a ^ \{l - 1} + b ^ l )
$$

ここで$σ$の引数を重み付き入力$z ^ l$と呼ぶことにする

$$
z ^ l \equiv w ^ l a ^ \{l - 1} + b ^ l
$$